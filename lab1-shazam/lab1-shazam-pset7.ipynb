{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hHVaOhbPASPY"
   },
   "source": [
    "# Shazam for ES 156 (Spring 2019).\n",
    "\n",
    "**Acknowledgments: ** several of the provided functions are taken the https://github.com/worldveil/dejavudataset, all credits to that repo. We also acknowledge Prof. Paul Cuff (Princeton) whose lab is the basis for this lab's structure.\n",
    "\n",
    "# Introduction\n",
    "How does Shazam work?\n",
    "\n",
    "Shazam is a popular app for recognizing a song from a short audio sample. Shazam started its operations in the year 2000, well before smartphones came out. Back then, you would have called a number on you (dumb) mobile phone, and song identification would be made directly from the mobile phone's microphone. The name of the song would then be texted back to you.\n",
    "\n",
    "The goal of this programming exercise is for you to implement a simplified version of Shazam. The algorithm behind Shazam is essentially a sequence of Fourier transforms taken over different time windows to form a \"spectrogram\" of an audio snippet. Only the largest values of the spectrogram are kept. This is done to speed up the matching with a song in a database and to provide robustness against noise. A special type of hashing algorithm is then used to match the spectrogram peaks of the audio recorded with your phone against examples in a database.\n",
    "\n",
    "Before starting the programming exercise, give this paper a quick look:\n",
    "https://www.ee.columbia.edu/~dpwe/papers/Wang03-shazam.pdf\n",
    "\n",
    "This is the original \"Shazam\" paper, where the full algorithm is described in significant detail. We won't implement the whole thing (in particular, we will skip the hashing part), but hopefully by the end of the notebook you will have a solid idea of how Shazam works.\n",
    "\n",
    "There are a lot of moving parts in this code, but don't be indimidated! If you end up getting stuck or hung-up on something, let the teaching staff know. As usual, we are excited to help you. We also appreciate your feedback and suggestions. Finally, if you find a way of improving the algorithm below, please let us know!\n",
    "\n",
    "# Preliminaries: Importing packages and data cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JbMShiNUASPb"
   },
   "source": [
    "# Imports\n",
    "First we start by importing a few packages we will use. You may have to install some of these packages in your anaconda python setup. Please contact the teaching staff if you run into issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ZJlRzfNlASPd"
   },
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from scipy import signal\n",
    "from operator import itemgetter\n",
    "import pyaudio\n",
    "import numpy as np\n",
    "import utils\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import operator\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ghaRR4ypASPk"
   },
   "source": [
    "# Settings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K0WctsFjASPm"
   },
   "source": [
    "We now define a few parameters that will be used throughout the lab. These values define different aspects of how the (real-valued) audio signal will be sampled and transformed into a discrete-time signal. The main components are the number of audio channels (2 -- left and righ), the sampling rate (44.1 KHz), the down sampling factor, and the window size of the FFT (recall that the FFT is just an algorithm that implements the DFT). These value are fairly standard in audio signal processing.\n",
    "\n",
    "Parameters to pay attention to: \n",
    "1. Fan Value: The way Shazam's hashing function works is by hashing *pairs* of peaks in the audio signal. This means the song's fingerprints are determined not just individual peaks, but also by the differences between peak pairs. (More on this in part 6). The fan value is the maximum number of pairs each peak can be part of. Therefore, it limits the number of pairs (and therefore fingerprints) we have for the signal. \n",
    "\n",
    "2. Overlap Ratio: When we take the spectrogram of our signal, we want there to be an overlap between our blocks of FFTs. The overlap ratio is the ratio of the number of samples that overlap to the number of samples in each FFT block. In other words, # of overlapping samples = # of samples per block * overlap ratio \n",
    "\n",
    "Note:\n",
    "\n",
    "- Tuning these values will (likely) change the system's performance. You can try to play around with the parameters but, for your first pass, leave the values below.\n",
    "\n",
    "- Beware cautioned that there is often a tradeoff between system accuracy and system efficiency if/when you attempt to tune these values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "NOn2aC8FASPn"
   },
   "outputs": [],
   "source": [
    "FORMAT = pyaudio.paInt16\n",
    "\n",
    "'''\n",
    "Number of audio channels in the recording\n",
    "'''\n",
    "CHANNELS = 2\n",
    "\n",
    "'''\n",
    "Original sample rate of the recordings\n",
    "'''\n",
    "SAMPLE_RATE = 44100\n",
    "\n",
    "\n",
    "'''\n",
    "Sampling rate (after downsampling)\n",
    "'''\n",
    "FS = 8000\n",
    "\n",
    "'''\n",
    "Factor by which the original signal will be downsampled\n",
    "'''\n",
    "DECIMATION_FACTOR = SAMPLE_RATE/FS\n",
    "\n",
    "'''\n",
    "Size of the FFT window, affects frequency granularity (we saw this in class!)\n",
    "'''\n",
    "WINDOW_SIZE = 1024\n",
    "\n",
    "'''\n",
    "Degree to which a fingerprint can be paired with its neighbors --\n",
    "higher will cause more fingerprints, but potentially better accuracy.\n",
    "'''\n",
    "FAN_VALUE = 15\n",
    "\n",
    "'''\n",
    "Ratio by which each window overlaps the previous and next window -- \n",
    "higher will cause more fingerprints, but higher granularity of offset matching\n",
    "'''\n",
    "OVERLAP_RATIO = 0.5\n",
    "\n",
    "path = os.getcwd()\n",
    "warnings.filterwarnings(\"ignore\", message=\"divide by zero encountered in log10\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-x0RtDOrASPq"
   },
   "source": [
    "## Part 1: Data import and cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x5Q4kGH-ASPr"
   },
   "source": [
    "Next, we will first construct an initial library of songs by doing the following:\n",
    "1. Open each song in our raw mp3s folder; \n",
    "2. Extract the data from each of the 2 channels.\n",
    "\n",
    "The recordings are all sampled at 44.1 KHz. (We will downsample in the next section) \n",
    "\n",
    "The dictionary `SongDb` will store the audio data. So `SongDb[s]` returns 2 arrays that contains the data for the first and second channel (i.e., the left and right channel) of the song `s`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "F40WfHe5ASPs",
    "outputId": "18f638fe-0752-4eac-de20-706a4a49811d"
   },
   "outputs": [],
   "source": [
    "# Database with key=songname, value=[channel1, channel2] \n",
    "SongDb = {}\n",
    "\n",
    "#Goes through mp3s folder and adds each song to database\n",
    "for filename in os.listdir(path + \"/mp3s/\"):\n",
    "    audiofile = AudioSegment.from_file(path + \"/mp3s/\" + filename) \n",
    "    data = np.fromstring(audiofile._data, np.int16)\n",
    "    channels = []\n",
    "    for chn in range(audiofile.channels):\n",
    "        channels.append(data[chn::audiofile.channels])\n",
    "    SongDb[filename[:-3]] = channels\n",
    "    print \"Added to song database: \" + str(filename[:-4])\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AbnvZZsZASP0"
   },
   "source": [
    "# Part 2: Preprocessing the Signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RmwTwoSpASP1"
   },
   "source": [
    "Before we create fingerprints for the songs, we must preprocess the data as follows:\n",
    "1. Combine the two data channels by: \n",
    "    - Taking their mean \n",
    "    - Subtracting mean to eliminate the cluster of peaks at frequency f = 0\n",
    "2. Downsampling the signal so that we don't have an excess of data\n",
    "\n",
    "The function Preprocess() should take as input a song's channels, and return the processed signal x above.\n",
    "\n",
    "The dictionary `ProcessedDb` will store the processed audio data. So `ProcessedDb[s]` returns a single array of the combined, processed data for the song `s`.\n",
    "\n",
    "We do the pre-processing for you, so you can focus on the singal processing aspects of the problem. Nevertheless, please make sure you understand the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "VqsVKvoGASP2"
   },
   "outputs": [],
   "source": [
    "def Preprocess(channels):\n",
    "    channel1 = channels[0]\n",
    "    channel2 = channels[1]\n",
    "    channelmean = ((channel1 + channel2)/2 - np.mean(channel1 + channel2))\n",
    "    resampled = signal.decimate(channelmean, DECIMATION_FACTOR)\n",
    "    return resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "0iBX_KddASP5",
    "outputId": "d954fa22-0fae-48b6-c6a1-d96cfd16e0ec"
   },
   "outputs": [],
   "source": [
    "# Database with key=songname, value=processed signal\n",
    "ProcessedDb = {}\n",
    "\n",
    "\n",
    "#Processes each song and adds it to ProcessedDb\n",
    "#Prints table of number of samples in for each song\n",
    "print '{0:65}{1:22}{2:20}\\n'.format('Song Name', 'Original #Samples',  'Processed #Samples')\n",
    "for song, sig in SongDb.items():\n",
    "    processed = Preprocess(sig)\n",
    "    ProcessedDb[song] = processed\n",
    "    original_duration = len(sig[0])\n",
    "    processed_duration = len(processed)\n",
    "    print '{0:50}{1:32d}{2:20d}'.format(song, original_duration, processed_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pIMCrtPXASP7"
   },
   "source": [
    "\n",
    "# Part 3: construct spectrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e3rFs090ASP8"
   },
   "source": [
    "- Now we want to construct the spectrogram of this signal. A (very) useful tool for this is the matplotlib.mlab spectrogram function. We want to specify the following parameters:\n",
    "    1. noverlap: the number of samples that will overlap between adjacent chunks\n",
    "    2. nfft: is the length of the fft you would like to take (can be the same as window)\n",
    "    3. fs: the sampling rate of the signal\n",
    "\n",
    "You can get the full documentation for the matblotlib.mlab spectrogram function here https://matplotlib.org/api/mlab_api.html#matplotlib.mlab.specgram)\n",
    "\n",
    "\n",
    "- After getting the spectrogram: \n",
    "    1. it is a good idea to use the log of its magnitudes (scaled by a constant factor of ~10) instead of just the raw magnitudes \n",
    "    2. It is also a good idea to set values returned as ±∞ (from divide by zeros) as 0 \n",
    "    \n",
    "\n",
    "#### Problem 1: implement a function called `getSpectrogram()` that takes a signal as an input, and returns an array with the log of the magnitudes as defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "PEY9aXbTASP9"
   },
   "outputs": [],
   "source": [
    "def getSpectrogram(signal):\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3XAeNw0BASP-"
   },
   "source": [
    "#### Problem 2: create a dictionary called `Spectrograms` that stores the spectrogram of each song, and plot the spectrogram of each song.\n",
    "\n",
    "Your plots should look something like this:\n",
    "<img src=\"spectrogram_examples.png\">\n",
    "\n",
    "Of course, use the colors and fonts for each plot that you like the most\n",
    "    \n",
    "- The dictionary `Spectrograms` will store the processed audio data. So `Spectrograms[s]` returns an array representing the spectrogram data for the song `s`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "agV_BmK2ASP_",
    "outputId": "99bb6748-145a-4204-9319-b85b77e9e32e"
   },
   "outputs": [],
   "source": [
    "''' TODO '''\n",
    "# Database with key=songname, value=spectrogram\n",
    "Spectrograms = {}\n",
    "\n",
    "# Gets the spectrogram for each song and adds it to the Spectrograms database\n",
    "# Plots each spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6NuUb30xASQD"
   },
   "source": [
    "\n",
    "# Part 4: spectrogram local peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LYgKeRTFASQE"
   },
   "source": [
    "The algorithm implemented by Shazam does not keep the entire spectrogram of the data. It would be to computationally intensive to match a song with w reference spectrogram by searching accross the entire time-frequency pairs. Moreover, the signal collected by the microphone on your phone is very noisy, so a lot of information in the spectrogram is useless. Instead, we only keep the largest (peak) values of the spectrogram. Those are the largest frequency components that, hopefully, can be clearly distinguished from the noise.\n",
    "\n",
    "Next, we will implement a function that gets the local peaks of the spectrogram and plot them.\n",
    "    - We use our get_2D_peaks() function defined in utils.py. \n",
    "    - This function will take a spectrogram as input. \n",
    "    - Recall that, by now, your spectrograms are saved in a dictionary.\n",
    "    - The output is a triple of: \n",
    "        1. the array frequency indices of each peak\n",
    "        2. the array of time indices of each peak\n",
    "        3. the array of peaks as (frequency value ,time value) \n",
    "    \n",
    "    \n",
    "\n",
    "- The dictionary `Peaks` will store the processed audio data. So `Peaks[s]` returns an array of the local peaks for the song `s` where each peak is a tuple of (frequency value, time value) \n",
    "\n",
    "#### Problem 3: using the function `utils.get_2D_peaks`, plot the peaks of the spectrogram on a time-frequency plot. for each spectrogram in the disctionary `Spectrograms`. Store the corresponding peak values in a dictionary `Peaks`.\n",
    "\n",
    "Your plots should look something like this (the colors, fonts, etc. are, of course, up to you).\n",
    "\n",
    "<img src=\"2dpeaks_examples.png\">\n",
    "\n",
    "The `Peaks` dictionary will play the role of the database that Shazam has. Later on, you will try to match the spectrogram peaks in a recording with the peaks in the dictionary `Peaks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Rk3mR3TYASQF",
    "outputId": "22ae0997-18d2-407d-edfa-ba631fed6b3e"
   },
   "outputs": [],
   "source": [
    "''' TODO '''\n",
    "# Database with key=songname, value=array of local peaks\n",
    "Peaks = {}\n",
    "\n",
    "# Gets the local peaks for each song and adds it to the Peaks database\n",
    "# Plots the peaks over the original spectrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fgnbBnVRASQM"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "# Part 5: peak pairing and table construction\n",
    "\n",
    "How can the peaks in our `Peaks` dictionary be matched against a recording? One of the main challenges is that our peaks are timestamped, and we don't know exactly to which instant of the song the audio recording that we have corresponds to. Finding this offset can be computationally intensive. Shazam deals with this time offset issue in a very clever way: instead of matching peaks, it looks at the **time difference** between spectrogram peaks.\n",
    "\n",
    "The final stage in creating our song fingerprints is to find pairs of peaks, and record a) their respective frequencies and b) the time difference between them. Peak pairs should meet the following constraints: \n",
    "    1. The second peak must occur within a certain time interval after the first peak\n",
    "    2. Each peak can only be part of a certain number of pairs (this pair limit is defined in our globals as FAN_VALUE)\n",
    "    \n",
    "\n",
    "#### Problem 5: Creat the function getPairs() should take the peaks of a given song and return the peak pairs defined above. This corresponds to the processing done when we record an audio with your phone.\n",
    "\n",
    "\n",
    "#### Problem 6: Create a dictionary `LookUpTable` that stores the peak pairs and their corresponding songs. So `LookUpTable[p]` returns the title of the song containing peak pair `p`. This function emulates the database search done by Shazam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "JsQK3j1nASQN"
   },
   "outputs": [],
   "source": [
    "''' TODO '''\n",
    "def getPairs(peaks):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "byJcVZSGASQQ",
    "outputId": "4cc56735-a743-47d3-e7b1-c8ad00f66879"
   },
   "outputs": [],
   "source": [
    "''' TODO '''\n",
    "# Database with key=fingerprint (f1, f2, tdelta), value=songname\n",
    "LookUpTable = {}\n",
    "\n",
    "# Get fingerprints for each song stores them in the LookUpTable database\n",
    "# Prints a sample of the LookUpTable entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eeH1RSoAASQT"
   },
   "source": [
    "# Part 6: test from files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "15n934DhASQT"
   },
   "source": [
    "#### Problem 6: Now we can test your Shazam! *(just run the code provided)*\n",
    "- You can access mp3s pulled from youtube video recordings of these songs are stored in the folder test_mp3s\n",
    "- Take a short snippet of these recordings and run the same fingerprinting process to getting peak pairs\n",
    "- Match the pairs against our peak pairs LookUpTable and see if you match the correct song!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Create database of test songs*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "rFNko-VFASQU",
    "outputId": "49bdb7fe-ac5d-4ebd-c4b4-a227176f6a45"
   },
   "outputs": [],
   "source": [
    "# Database with key=songname, value=[channel1, channel2] for a snippet of the song\n",
    "TestDb = {}\n",
    "\n",
    "# Goes through test_mp3s folder and adds a snippet of each song to database \n",
    "for filename in os.listdir(path + \"/test_mp3s/\"):\n",
    "    audiofile = AudioSegment.from_file(path + \"/test_mp3s/\" + filename) \n",
    "    data = np.fromstring(audiofile._data, np.int16)[SAMPLE_RATE*60:SAMPLE_RATE*75]\n",
    "    channels = []\n",
    "    for chn in range(audiofile.channels):\n",
    "        channels.append(data[chn::audiofile.channels])\n",
    "    TestDb[filename] = channels\n",
    "    print \"Added to test database: \" + str(filename)\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Run Tests*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "FucZ-8xSASQW",
    "outputId": "7e603fbc-a3b5-4548-eb7c-d8d42961c040"
   },
   "outputs": [],
   "source": [
    "# Goes through test snippets and runs same fingerprinting process\n",
    "# Prints out the number of matches for each song and confidence of prediction\n",
    "for test in TestDb.keys():\n",
    "    print '\\033[1mTesting: ' + test + '\\033[0m \\n'\n",
    "    Matches = {}\n",
    "    for song in SongDb.keys():\n",
    "        Matches[song] = 0\n",
    "    channels = TestDb[test]\n",
    "    preprocessed = Preprocess(channels)\n",
    "    spectrogram = getSpectrogram(preprocessed)\n",
    "    _, _, peaks = utils.get_2D_peaks(spectrogram)\n",
    "    pairs = getPairs(peaks)\n",
    "    for p in pairs:\n",
    "        match = LookUpTable.get(p, None)\n",
    "        if match:\n",
    "            Matches[match] += 1\n",
    "    prediction, count = max(Matches.items(), key=itemgetter(1))\n",
    "    for k,v in Matches.items():\n",
    "        if k == prediction:\n",
    "            print '\\033[1m{0:50} ==> {1:10d} \\033[0m'.format(k, v)\n",
    "        else:\n",
    "            print '{0:50} ==> {1:10d}'.format(k, v)\n",
    "    confidence = str(float(count)/sum(Matches.values())*100)[:5] + \"%\"\n",
    "    print '\\033[1m{0:10}: {1:10}\\033[0m\\n\\n'.format('Confidence', confidence)\n",
    "    prediction = max(Matches.items(), key=itemgetter(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "PgbuXwWfASQY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "sFb2UCwdASQa"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "lab1-shazam.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
